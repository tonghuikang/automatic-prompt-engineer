{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c73ef23",
   "metadata": {},
   "source": [
    "# Automatic Prompt Engineering for classification\n",
    "\n",
    "Given (text -> label), this notebook generates and optimizes system and user prompts.\n",
    "\n",
    "This is how the text will be labelled\n",
    "- (system prompt, user prompt prefix + text + user prompt suffix) -Haiku-> bot response -function-> label\n",
    "- The function will be defined by you (it could be just a string match)\n",
    "\n",
    "The notebook will produce\n",
    "- the system prompt\n",
    "- the user prompt prefix\n",
    "- the user prompt suffix\n",
    "\n",
    "To use this tool, you will need\n",
    "- an Anthropic API key\n",
    "- a dataset (text -> label)\n",
    "- define the function bot_response -> label\n",
    "- describe the expected bot_response that Haiku should produce\n",
    "\n",
    "This is how prompt tuning is done\n",
    "- Sample from the full dataset.\n",
    "- Haiku takes in (system prompt, user prompt prefix + text + user prompt suffix) and produces bot_response.\n",
    "- The function takes in bot_response and produces the label. The (text -> label) process is the forward pass.\n",
    "- Sample from the mistakes.\n",
    "- Opus takes in the mistakes and summarizes the mistakes (this is the gradient).\n",
    "- Opus takes in the gradient (gradient) and the current prompts (model parameters) updates the prompts.\n",
    "- Repeat.\n",
    "\n",
    "You will need to have these Python modules installed\n",
    "- pandas\n",
    "- anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c074e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import textwrap\n",
    "import collections\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8e450",
   "metadata": {},
   "source": [
    "# Use your Anthropic API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c127a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "# anthropic_api_key = \"sk-ant-\"\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f546dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-ant-api03-Mbs\n"
     ]
    }
   ],
   "source": [
    "print(anthropic_api_key[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6130e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARALLEL_FORWARD_PASS_API_CALLS = 100  # see https://docs.anthropic.com/claude/reference/rate-limits\n",
    "NUM_SAMPLES_FORWARD_PASS_FOR_EACH_LABEL = 100\n",
    "NUM_SAMPLES_MISTAKE_GRADIENT_CALCULATION_FOR_EACH_LABEL = 10\n",
    "NUM_SAMPLES_CORRECT_GRADIENT_CALCULATION_FOR_EACH_LABEL = 5\n",
    "NUM_ITERATIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33cf13",
   "metadata": {},
   "source": [
    "# Define the dataset here\n",
    "You will need to edit this if your task is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba53aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://www.kaggle.com/c/quora-insincere-questions-classification/data\n",
    "df = pd.read_csv(\"qiqc_truncated.csv\")\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1df0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    df[df[\"target\"] == 1].sample(100),\n",
    "    df[df[\"target\"] == 0].sample(100),\n",
    "], ignore_index=True).sample(frac=1)\n",
    "\n",
    "# you can also just define the dataset with code\n",
    "dataset = list(zip(df[\"question_text\"], df[\"target\"].map({0: \"sincere\", 1: \"insincere\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "031f9dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'insincere': 100, 'sincere': 100})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the number of types of labels is small\n",
    "# prefer descriptive labels\n",
    "collections.Counter(label for _, label in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9380f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Is it generally acceptable if someone grew up in the USA and considers Mediterranean Europeans like French or Spaniards not white, but rather black, because they are not Germanic like Dutch or Scandinavian people?',\n",
       " 'insincere')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]  # should be tuple[string, label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9c32f",
   "metadata": {},
   "source": [
    "# Define your task here\n",
    "You will need to edit this if your task is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdccd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_final_layer(final_layer_value):\n",
    "    if \"In conclusion, this question is sincere.\" in final_layer_value:\n",
    "        return \"sincere\"\n",
    "    return \"insincere\"  # note that this is the default predicted label if things go wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334c7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_UPDATE_SYSTEM_PROMPT = \"\"\"\n",
    "You will write a set of prompts for an LLM to classify where a question is insincere.\n",
    "\n",
    "The LLM will take the following input\n",
    "- system_prompt\n",
    "- user_prompt_prefix + question + user\n",
    "\n",
    "The LLM is expected to produce the following output\n",
    "- reasoning on whether the question is insincere\n",
    "- with an ending \"In conclusion, this question is insincere.\" or \"In conclusion, this question is sincere.\"\n",
    "\n",
    "A function will take the LLM output and check for the exact string \"In conclusion, this question is sincere\".\n",
    "If the string appears in the LLM output, the question will be predicted to be insincere.\n",
    "\n",
    "Please remember include the instruction to produce the exact string at the end of the LLM output.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fdc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually Opus is good enough to produce working prompts\n",
    "model_parameters = {\n",
    "    \"system_prompt\": \"\",\n",
    "    \"user_prompt_prefix\": \"\",\n",
    "    \"user_prompt_suffix\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b3a15",
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "\n",
    "This should be general enough for classification tasks with a small number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a927c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_and_labels(dataset):\n",
    "    dataset = [data for data in dataset]\n",
    "    random.shuffle(dataset)\n",
    "    label_set = set(label for _,label in dataset)\n",
    "\n",
    "    sampled_dataset = []\n",
    "    for target_label in label_set:\n",
    "        dataset_with_label = [(data, label) for data, label in dataset if label == target_label]\n",
    "        sampled_dataset += dataset_with_label[:NUM_SAMPLES_FORWARD_PASS_FOR_EACH_LABEL]\n",
    "    random.shuffle(sampled_dataset)\n",
    "\n",
    "    return [data for data, _ in sampled_dataset], [label for _, label in sampled_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4cbd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_layer(sample, model_parameters):\n",
    "    \n",
    "    user_message = model_parameters[\"user_prompt_prefix\"] + sample + model_parameters[\"user_prompt_suffix\"]\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        system=model_parameters[\"system_prompt\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_message}]}],\n",
    "        timeout=10\n",
    "    )\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576c5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(samples, model_parameters):\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL_FORWARD_PASS_API_CALLS) as executor:\n",
    "        final_layer_values = executor.map(compute_final_layer, samples, [model_parameters]*len(samples))\n",
    "\n",
    "    final_layer_values = list(final_layer_values)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL_FORWARD_PASS_API_CALLS) as executor:\n",
    "        predicted_labels = executor.map(predict_from_final_layer, final_layer_values)\n",
    "\n",
    "    predicted_labels = list(predicted_labels)\n",
    "\n",
    "    return final_layer_values, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f701285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(samples, final_layer_values, predicted_labels, actual_labels, metrics):\n",
    "\n",
    "    system_message = \"You will provide a concise summary of the mistakes in the classification.\"\n",
    "    \n",
    "    mistake_counts = collections.defaultdict(int)\n",
    "    correct_counts = collections.defaultdict(int)\n",
    "\n",
    "    user_message = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        Please summarize the mistakes in the classification where predicted_label != actual_label\n",
    "        \n",
    "        The current metrics is {str(metrics)}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    for sample, final_layer_value, predicted_label, actual_label in zip(\n",
    "        samples, final_layer_values, predicted_labels, actual_labels\n",
    "    ):\n",
    "        if predicted_label == actual_label:\n",
    "            if correct_counts[actual_label] > NUM_SAMPLES_CORRECT_GRADIENT_CALCULATION_FOR_EACH_LABEL:\n",
    "                continue\n",
    "            correct_counts[actual_label] += 1\n",
    "        else:\n",
    "            if mistake_counts[actual_label] > NUM_SAMPLES_MISTAKE_GRADIENT_CALCULATION_FOR_EACH_LABEL:\n",
    "                continue\n",
    "            mistake_counts[actual_label] += 1\n",
    "        \n",
    "        user_message += textwrap.dedent(\n",
    "            f\"\"\"\n",
    "            <sample>{sample}<\\sample>\n",
    "            \n",
    "            <final_layer_value>{final_layer_value}<\\final_layer_value>\n",
    "\n",
    "            <predicted_label>{predicted_label}<\\predicted_label>\n",
    "            \n",
    "            <actual_label>{actual_label}<\\actual_label>\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_message}]}]\n",
    "    )\n",
    "    \n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a50c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_parameters(gradient, model_parameters, metrics):\n",
    "\n",
    "    system_message = PROMPT_UPDATE_SYSTEM_PROMPT\n",
    "\n",
    "    user_message = textwrap.dedent(f\"\"\"    \n",
    "    The current metrics is {str(metrics)}\n",
    "\n",
    "    This the current set of prompts\n",
    "    <system_prompt>\n",
    "    {model_parameters['system_prompt']}\n",
    "    </system_prompt>\n",
    "\n",
    "    <user_prompt_prefix>\n",
    "    {model_parameters['user_prompt_prefix']}\n",
    "    </user_prompt_prefix>\n",
    "\n",
    "    <user_prompt_suffix>\n",
    "    {model_parameters['user_prompt_suffix']}\n",
    "    </user_prompt_suffix>\n",
    "\n",
    "    This is the feedback on the prompt\n",
    "    <feedback>\n",
    "    {gradient}\n",
    "    </feedback>\n",
    "\n",
    "    Please reply in the following format\n",
    "    \n",
    "    <system_prompt>\n",
    "    (the new system prompt here)\n",
    "    </system_prompt>\n",
    "\n",
    "    <user_prompt_prefix>\n",
    "    (the new user prompt prefix here)\n",
    "    </user_prompt_prefix>\n",
    "\n",
    "    <user_prompt_suffix>\n",
    "    (the new user prompt suffix here)\n",
    "    </user_prompt_suffix>\n",
    "    \"\"\")\n",
    "    \n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        system=system_message,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_message}]}],\n",
    "    )\n",
    "    \n",
    "    bot_message = message.content[0].text\n",
    "\n",
    "    match_system_prompt = re.search(r'<system_prompt>(.*?)</system_prompt>', bot_message, re.DOTALL)\n",
    "    match_user_prompt_prefix = re.search(r'<user_prompt_prefix>(.*?)</user_prompt_prefix>', bot_message, re.DOTALL)\n",
    "    match_user_prompt_suffix = re.search(r'<user_prompt_suffix>(.*?)</user_prompt_suffix>', bot_message, re.DOTALL)    \n",
    "    \n",
    "    model_parameters = {\n",
    "        \"system_prompt\": match_system_prompt.group(1) if match_system_prompt else \"\",\n",
    "        \"user_prompt_prefix\": match_user_prompt_prefix.group(1) if match_user_prompt_prefix else \"\",\n",
    "        \"user_prompt_suffix\": match_user_prompt_suffix.group(1) if match_user_prompt_suffix else \"\",\n",
    "    }\n",
    "    \n",
    "    return model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4f661",
   "metadata": {},
   "source": [
    "# Display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040e056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_labels, actual_labels):\n",
    "    metrics = {}\n",
    "    for label in set(actual_labels):\n",
    "        metrics[f\"{label}_precision\"] = precision_score(\n",
    "            [actual_label == label for actual_label in actual_labels],\n",
    "            [predicted_label == label for predicted_label in predicted_labels],\n",
    "            zero_division = 0,\n",
    "        )\n",
    "        metrics[f\"{label}_recall\"] = recall_score(\n",
    "            [actual_label == label for actual_label in actual_labels],\n",
    "            [predicted_label == label for predicted_label in predicted_labels],\n",
    "        )        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04eaf8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_prompt_history(model_parameters_history, gradient_history, metrics_history):\n",
    "\n",
    "    iteration_data_all = []\n",
    "\n",
    "    for model_parameter, gradient, metrics in itertools.zip_longest(\n",
    "        model_parameters_history, gradient_history, metrics_history, fillvalue={}\n",
    "    ):\n",
    "        iteration_data = {}\n",
    "        for k,v in model_parameter.items():\n",
    "            iteration_data[k] = v\n",
    "        for k,v in metrics.items():\n",
    "            iteration_data[k] = v\n",
    "        if gradient:\n",
    "            iteration_data[\"gradient\"] = gradient\n",
    "        iteration_data_all.append(iteration_data)\n",
    "\n",
    "    df = pd.DataFrame(iteration_data_all).fillna(\"\")\n",
    "\n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    prompt_info_file_name = \"html_output/prompt-history-classification.html\"\n",
    "    with open(prompt_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            df.replace(\n",
    "                {r'\\n': '<br>'}, regex=True\n",
    "            ).style.set_table_styles(\n",
    "                [\n",
    "                    dict(selector=\"tr:nth-child(even)\", props=[(\"background-color\", \"#f2f2f2\")]),\n",
    "                    dict(selector=\"tr:nth-child(odd)\", props=[(\"background-color\", \"white\")]),\n",
    "                ]\n",
    "            ).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    link = f'<a href=\"{prompt_info_file_name}\" target=\"_blank\">{prompt_info_file_name}</a>'\n",
    "    display(HTML(link))\n",
    "    \n",
    "    \n",
    "def save_and_display_current_iteration(iteration_idx, samples, final_layer_values, predicted_labels, actual_labels):\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"sample\": samples,\n",
    "        \"final_layer_value\": final_layer_values,\n",
    "        \"predicted_label\": predicted_labels,\n",
    "        \"actual_label\": actual_labels,\n",
    "    })\n",
    "    \n",
    "    def highlight_diff(row):\n",
    "        if row['predicted_label'] == row['actual_label']:\n",
    "            return ['background-color: #90EE90'] * len(row)  # green\n",
    "        return ['background-color: #FFB6C1'] * len(row)  # red\n",
    "    \n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    iteration_info_file_name = f\"html_output/iteration-classification-{iteration_idx:03}.html\"\n",
    "    with open(iteration_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            df.replace(\n",
    "                {r'\\n': '<br>'}, regex=True\n",
    "            ).style.apply(highlight_diff, axis=1).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    link = f'<a href=\"{iteration_info_file_name}\" target=\"_blank\">{iteration_info_file_name}</a>'\n",
    "    display(HTML(link))\n",
    "    \n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    iteration_info_file_name = f\"html_output/iteration-classification-{iteration_idx:03}-diff.html\"\n",
    "    with open(iteration_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            df[df[\"predicted_label\"] != df[\"actual_label\"]].sort_values(\"actual_label\").replace(\n",
    "                {r'\\n': '<br>'}, regex=True\n",
    "            ).style.set_table_styles(\n",
    "                [\n",
    "                    dict(selector=\"tr:nth-child(even)\", props=[(\"background-color\", \"#f2f2f2\")]),\n",
    "                    dict(selector=\"tr:nth-child(odd)\", props=[(\"background-color\", \"white\")]),                    \n",
    "                ]\n",
    "            ).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    link = f'<a href=\"{iteration_info_file_name}\" target=\"_blank\">{iteration_info_file_name}</a>'\n",
    "    display(HTML(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138725e1",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1647bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-000.html\" target=\"_blank\">html_output/iteration-classification-000.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-000-diff.html\" target=\"_blank\">html_output/iteration-classification-000-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-001.html\" target=\"_blank\">html_output/iteration-classification-001.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-001-diff.html\" target=\"_blank\">html_output/iteration-classification-001-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-002.html\" target=\"_blank\">html_output/iteration-classification-002.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-002-diff.html\" target=\"_blank\">html_output/iteration-classification-002-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-003.html\" target=\"_blank\">html_output/iteration-classification-003.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-003-diff.html\" target=\"_blank\">html_output/iteration-classification-003-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-004.html\" target=\"_blank\">html_output/iteration-classification-004.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-004-diff.html\" target=\"_blank\">html_output/iteration-classification-004-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_parameters_history = [{k:v for k,v in model_parameters.items()}]\n",
    "gradient_history = []\n",
    "metrics_history = []\n",
    "\n",
    "for iteration_idx in range(NUM_ITERATIONS):\n",
    "    samples, actual_labels = get_samples_and_labels(dataset)\n",
    "\n",
    "    final_layer_values, predicted_labels = forward_pass(samples, model_parameters)\n",
    "    metrics = calculate_metrics(predicted_labels, actual_labels)\n",
    "    gradient = calculate_gradient(samples, final_layer_values, predicted_labels, actual_labels, metrics)\n",
    "    model_parameters = update_model_parameters(gradient, model_parameters, metrics)\n",
    "\n",
    "    metrics_history.append(metrics)\n",
    "    gradient_history.append(gradient)\n",
    "    model_parameters_history.append({k:v for k,v in model_parameters.items()})\n",
    "\n",
    "    save_and_display_prompt_history(model_parameters_history, gradient_history, metrics_history)    \n",
    "    save_and_display_current_iteration(iteration_idx, samples, final_layer_values, predicted_labels, actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe67012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
