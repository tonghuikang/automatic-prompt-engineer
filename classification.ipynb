{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c73ef23",
   "metadata": {},
   "source": [
    "# Automatic Prompt Engineering for classification\n",
    "\n",
    "Given only (text -> label), this notebook generates and optimizes system and user prompts.\n",
    "\n",
    "This is how classification is intended to be done.\n",
    "- (system prompt, user prompt prefix + text + user prompt suffix) -Haiku-> bot response -extract-> label\n",
    "\n",
    "The notebook will produce\n",
    "- the system prompt\n",
    "- the user prompt prefix\n",
    "- the user prompt suffix\n",
    "\n",
    "You can simply run this notebook with just\n",
    "- an Anthropic API key and an OpenAI API key\n",
    "\n",
    "If you want to change the classification task, you will need to\n",
    "- provide a dataset (text -> label)\n",
    "\n",
    "This is how prompt tuning is done\n",
    "- Sample from the full dataset.\n",
    "- Haiku takes in (system prompt, user prompt prefix + text + user prompt suffix) and produces model_response\n",
    "- Extract the label from the model_response.\n",
    "- Sample from the mistakes and the correct results.\n",
    "- o1-mini summarizes the mistakes and update the prompts (model parameters).\n",
    "- Repeat.\n",
    "\n",
    "You will need to have these Python modules installed\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- anthropic\n",
    "- openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c074e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import inspect\n",
    "import textwrap\n",
    "import collections\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import html\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8e450",
   "metadata": {},
   "source": [
    "# Use your Anthropic API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c127a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "# anthropic_api_key = \"sk-ant-\"\n",
    "anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c9b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-ant-api03-Mbs\n"
     ]
    }
   ],
   "source": [
    "print(anthropic_client.api_key[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "147ef4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-T0uvUYsHoQOag\n"
     ]
    }
   ],
   "source": [
    "print(openai_client.api_key[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6130e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARALLEL_FORWARD_PASS_API_CALLS = 100  # see https://docs.anthropic.com/claude/reference/rate-limits\n",
    "NUM_SAMPLES_FORWARD_PASS_FOR_EACH_LABEL = 100\n",
    "NUM_SAMPLES_MISTAKE_GRADIENT_CALCULATION_FOR_EACH_LABEL = 10\n",
    "NUM_SAMPLES_CORRECT_GRADIENT_CALCULATION_FOR_EACH_LABEL = 5\n",
    "NUM_ITERATIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33cf13",
   "metadata": {},
   "source": [
    "# Define the dataset here\n",
    "You will need to edit this if your task is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba53aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1000\n",
       "1    1000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://www.kaggle.com/c/quora-insincere-questions-classification/data\n",
    "df = pd.read_csv(\"qiqc_truncated.csv\")\n",
    "df[\"target\"] = df[\"target\"].astype(str)\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1df0784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100\n",
       "1    100\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    df[df[\"target\"] == value].sample(min(count, 100), random_state=42)\n",
    "    for value, count in df[\"target\"].value_counts().iteritems()\n",
    "], ignore_index=True).sample(frac=1, random_state=0)\n",
    "\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1909080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also just define the dataset with code\n",
    "dataset = list(zip(df[\"question_text\"], df[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e4e290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 100, '1': 100})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the number of types of labels is small\n",
    "# prefer descriptive labels to avoid giving the model mental gymnastics\n",
    "collections.Counter(label for _, label in dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9380f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Why does programming languages need to be conplicated and difficult to learn or even understand, why dont we make as simple as normal English?',\n",
       " '0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]  # should be tuple[string, label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c9c32f",
   "metadata": {},
   "source": [
    "# Define your task here\n",
    "You will need to edit this if your task is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdccd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_from_model_response(text):\n",
    "    pattern = r'<label>(.*?)</label>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fdc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually o1 is good enough to produce working prompts from nothing\n",
    "model_parameters = {\n",
    "    \"system_prompt\": \"\",\n",
    "    \"user_prompt_prefix\": \"\",\n",
    "    \"user_prompt_suffix\": \"\",\n",
    "}\n",
    "\n",
    "token_counts = defaultdict(int)\n",
    "token_costs = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653db4c2",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "If you want to use a different model, you can configure here this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a099cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_model_response(text, model_parameters):\n",
    "    \n",
    "    user_message = model_parameters[\"user_prompt_prefix\"] + text + model_parameters[\"user_prompt_suffix\"]\n",
    "    \n",
    "    message = anthropic_client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_message}]}],\n",
    "        system=model_parameters[\"system_prompt\"],\n",
    "        timeout=10\n",
    "    )\n",
    "    token_counts[\"haiku_input\"] += message.usage.input_tokens\n",
    "    token_counts[\"haiku_output\"] += message.usage.output_tokens\n",
    "    token_costs[\"haiku_input\"] += message.usage.input_tokens * 0.25 * 1e-6\n",
    "    token_costs[\"haiku_output\"] += message.usage.output_tokens * 1.25 * 1e-6\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b3a15",
   "metadata": {},
   "source": [
    "# Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a927c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_and_labels(dataset):\n",
    "    dataset = [data for data in dataset]\n",
    "    random.shuffle(dataset)\n",
    "    label_set = set(label for _,label in dataset)\n",
    "\n",
    "    sampled_dataset = []\n",
    "    for target_label in label_set:\n",
    "        dataset_with_label = [(data, label) for data, label in dataset if label == target_label]\n",
    "        sampled_dataset += dataset_with_label[:NUM_SAMPLES_FORWARD_PASS_FOR_EACH_LABEL]\n",
    "    random.shuffle(sampled_dataset)\n",
    "\n",
    "    return [data for data, _ in sampled_dataset], [label for _, label in sampled_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "576c5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(texts, model_parameters):\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL_FORWARD_PASS_API_CALLS) as executor:\n",
    "        model_response = executor.map(compute_model_response, texts, [model_parameters]*len(texts))\n",
    "\n",
    "    model_response = list(model_response)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL_FORWARD_PASS_API_CALLS) as executor:\n",
    "        predicted_labels = executor.map(extract_from_model_response, model_response)\n",
    "\n",
    "    predicted_labels = list(predicted_labels)\n",
    "\n",
    "    return model_response, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a50c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_parameters(texts, model_responses, \n",
    "                            predicted_labels, actual_labels,\n",
    "                            model_parameters, metrics):\n",
    "\n",
    "    mistake_counts = collections.defaultdict(int)\n",
    "    correct_counts = collections.defaultdict(int)\n",
    "\n",
    "    user_message = textwrap.dedent(\n",
    "        f\"\"\"\n",
    "        You are given\n",
    "        - a set of (text, model response, extracted label, expected label)\n",
    "            - extracted label may be None if it is not found in the model response\n",
    "        - the current set of prompts (which may be empty) for an LLM\n",
    "\n",
    "        You will improve the prompts so that the LLM will predict the expected label.\n",
    "        You might need to guess what each label means.\n",
    "\n",
    "        The LLM input has the following parameters. Make use of all the parameters.\n",
    "        - {list(model_parameters.keys())}\n",
    "        \n",
    "        This is how the parameters are used by the LLM\n",
    "        {inspect.getsource(compute_model_response)}\n",
    "\n",
    "        Please ensure that the prompt contains these following instructions\n",
    "        - The label should have the same exact text as the actual labels\n",
    "        - The information on what the set of labels are\n",
    "        - A small number of example inputs and outputs\n",
    "        - The LLM should only classify the text. The LLM should not respond to the text or decline classifying the text.\n",
    "        - The LLM should provide a concise reasoning. The reasoning should happen before the label.\n",
    "        - The response should always end with Label: <label>{{label}}</label>\n",
    "            - Note that the label needs exactly the text in the expected label\n",
    "            - Note that you need the html tags\n",
    "        \n",
    "        The current metrics is {str(metrics)}.\n",
    "        Put more focus on the worst performing metric.\n",
    "        \"\"\"\n",
    "    ) + \"\\n\\n\\n\"\n",
    "    \n",
    "    for text, model_response, predicted_label, actual_label in zip(\n",
    "        texts, model_responses, predicted_labels, actual_labels\n",
    "    ):\n",
    "        correctness_verdict = \"\"\n",
    "        actual_labels_set = set(actual_labels)\n",
    "        if predicted_label not in actual_labels_set:\n",
    "            correctness_verdict = \"This label could not be extracted or does not belong to one of the actual labels.\"\n",
    "            if mistake_counts[None] > NUM_SAMPLES_MISTAKE_GRADIENT_CALCULATION_FOR_EACH_LABEL:\n",
    "                continue\n",
    "            mistake_counts[None] += 1\n",
    "        elif predicted_label == actual_label:\n",
    "            correctness_verdict = \"This prediction is correct.\"\n",
    "            if correct_counts[actual_label] > NUM_SAMPLES_CORRECT_GRADIENT_CALCULATION_FOR_EACH_LABEL:\n",
    "                continue\n",
    "            correct_counts[actual_label] += 1\n",
    "        else:\n",
    "            correctness_verdict = \"This prediction is incorrect.\"\n",
    "            if mistake_counts[actual_label] > NUM_SAMPLES_MISTAKE_GRADIENT_CALCULATION_FOR_EACH_LABEL:\n",
    "                continue\n",
    "            mistake_counts[actual_label] += 1\n",
    "            \n",
    "        user_message += textwrap.dedent(\n",
    "            f\"\"\"\n",
    "            <datapoint>\n",
    "                {correctness_verdict}\n",
    "            \n",
    "                <text>{text}</text>\n",
    "\n",
    "                <model_response>{model_response}</model_response>\n",
    "\n",
    "                <predicted_label>{predicted_label}</predicted_label>\n",
    "\n",
    "                <actual_label>{actual_label}</actual_label>\n",
    "            \n",
    "            </datapoint>\n",
    "            \"\"\"\n",
    "        ) + \"\\n\\n\"\n",
    "    \n",
    "    user_message += \"\\n\\n\\nThis the current set of prompts\\n\"\n",
    "    \n",
    "    for model_parameter_key, current_model_parameter_value in model_parameters.items():\n",
    "        user_message += textwrap.dedent(f\"\"\"\n",
    "        <{model_parameter_key}>\n",
    "        {current_model_parameter_value}\n",
    "        </{model_parameter_key}>\n",
    "        \"\"\") + \"\\n\\n\"\n",
    "\n",
    "    user_message += textwrap.dedent(f\"\"\"\n",
    "    Your reply (not the LLM you are tuning prompts for) should include the following\n",
    "    \n",
    "    A summary of the current performance\n",
    "    \n",
    "    The key mistakes observed with some examples\n",
    "    \n",
    "    What are the proposed changes the prompt\n",
    "    \n",
    "    The prompts in the following format\n",
    "    \"\"\") + \"\\n\\n\"\n",
    "\n",
    "    for model_parameter_key in model_parameters.keys():\n",
    "        user_message += textwrap.dedent(f\"\"\"\n",
    "        <{model_parameter_key}>\n",
    "        the new {model_parameter_key} here\n",
    "        </{model_parameter_key}>\n",
    "        \"\"\") + \"\\n\\n\"        \n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"o1-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_message}]\n",
    "    )\n",
    "\n",
    "    token_counts[\"o1_input\"] += response.usage.prompt_tokens\n",
    "    token_counts[\"o1_output\"] += response.usage.completion_tokens\n",
    "    token_costs[\"o1_input\"] += response.usage.prompt_tokens * 3 * 1e-6\n",
    "    token_costs[\"o1_output\"] += response.usage.completion_tokens * 15 * 1e-6\n",
    "\n",
    "    parameters_update_response = response.choices[0].message.content\n",
    "\n",
    "    for model_parameter_key in model_parameters.keys():\n",
    "        groups = re.search(\n",
    "            fr'<{model_parameter_key}>(.*?)</{model_parameter_key}>',\n",
    "            parameters_update_response, re.DOTALL\n",
    "        )\n",
    "        model_parameters[model_parameter_key] = groups.group(1) if groups else \"\"\n",
    "    \n",
    "    return model_parameters, parameters_update_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4f661",
   "metadata": {},
   "source": [
    "# Display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040e056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predicted_labels, actual_labels):\n",
    "    metrics = {}\n",
    "    actual_labels_set = set(actual_labels)\n",
    "    for label in sorted(actual_labels_set):\n",
    "        metrics[f\"{label}_precision\"] = precision_score(\n",
    "            [actual_label == label for actual_label in actual_labels],\n",
    "            [predicted_label == label for predicted_label in predicted_labels],\n",
    "            zero_division = 0,\n",
    "        )\n",
    "        metrics[f\"{label}_recall\"] = recall_score(\n",
    "            [actual_label == label for actual_label in actual_labels],\n",
    "            [predicted_label == label for predicted_label in predicted_labels],\n",
    "        )\n",
    "    metrics[\"missing\"] = sum(\n",
    "        [predicted_label not in actual_labels_set for predicted_label in predicted_labels]\n",
    "    ) / len(predicted_labels)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04eaf8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_prompt_history(model_parameters_history, parameters_update_response_history, metrics_history):\n",
    "\n",
    "    iteration_data_all = []\n",
    "\n",
    "    for model_parameters, parameters_update_response, metrics in itertools.zip_longest(\n",
    "        model_parameters_history, parameters_update_response_history, metrics_history, fillvalue={}\n",
    "    ):\n",
    "        iteration_data = {}\n",
    "        for k,v in model_parameters.items():\n",
    "            iteration_data[k] = v\n",
    "        for k,v in metrics.items():\n",
    "            iteration_data[k] = f\"{v:.3f}\"\n",
    "        if parameters_update_response:\n",
    "            iteration_data[\"parameters_update_response\"] = parameters_update_response\n",
    "        iteration_data_all.append(iteration_data)\n",
    "\n",
    "    df = pd.DataFrame(iteration_data_all).fillna(\"\")\n",
    "\n",
    "    html_prefix = '''\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "    }\n",
    "    td, th {\n",
    "        border: 1px solid black;\n",
    "        padding: 5px;\n",
    "        vertical-align: top;\n",
    "    }\n",
    "    td {\n",
    "        white-space: pre-wrap;\n",
    "        font-family: monospace;\n",
    "    }\n",
    "    </style>\n",
    "    '''\n",
    "    \n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    prompt_info_file_name = \"html_output/prompt-history-classification.html\"\n",
    "    with open(prompt_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            html_prefix + df.replace(\n",
    "                {r'\\n': '__NEWLINE__'}, regex=True\n",
    "            ).applymap(str).applymap(html.escape).replace(\n",
    "                {'__NEWLINE__': '<br>'}, regex=True\n",
    "            ).style.set_table_styles(\n",
    "                [\n",
    "                    dict(selector=\"tr:nth-child(even)\", props=[(\"background-color\", \"#f2f2f2\")]),\n",
    "                    dict(selector=\"tr:nth-child(odd)\", props=[(\"background-color\", \"white\")]),\n",
    "                ]\n",
    "            ).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    link = f'<a href=\"{prompt_info_file_name}\" target=\"_blank\">{prompt_info_file_name}</a>'\n",
    "    display(HTML(link))\n",
    "        \n",
    "    \n",
    "def save_and_display_current_iteration(iteration_idx, texts, model_response, predicted_labels, actual_labels):\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"text\": texts,\n",
    "        \"model_response\": model_response,\n",
    "        \"predicted_label\": predicted_labels,\n",
    "        \"actual_label\": actual_labels,\n",
    "    })\n",
    "    \n",
    "    def highlight_diff(row):\n",
    "        if row['predicted_label'] == row['actual_label']:\n",
    "            return ['background-color: #90EE90'] * len(row)  # green\n",
    "        return ['background-color: #FFB6C1'] * len(row)  # red\n",
    "\n",
    "    html_prefix = '''\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <style>\n",
    "    table {\n",
    "        border-collapse: collapse;\n",
    "    }\n",
    "    td, th {\n",
    "        border: 1px solid black;\n",
    "        padding: 5px;\n",
    "        vertical-align: top;\n",
    "    }\n",
    "    td {\n",
    "        white-space: pre-wrap;\n",
    "        font-family: monospace;\n",
    "    }\n",
    "    </style>\n",
    "    '''    \n",
    "    \n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    iteration_info_file_name = f\"html_output/iteration-classification-{iteration_idx:03}.html\"\n",
    "    with open(iteration_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            html_prefix + df.replace(\n",
    "                {r'\\n': '__NEWLINE__'}, regex=True\n",
    "            ).applymap(str).applymap(html.escape).replace(\n",
    "                {'__NEWLINE__': '<br>'}, regex=True\n",
    "            ).style.apply(highlight_diff, axis=1).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    link = f'<a href=\"{iteration_info_file_name}\" target=\"_blank\">{iteration_info_file_name}</a>'\n",
    "    display(HTML(link))   \n",
    "    \n",
    "    os.makedirs(\"html_output\", exist_ok=True)\n",
    "    iteration_info_file_name = f\"html_output/iteration-classification-{iteration_idx:03}-diff.html\"\n",
    "    with open(iteration_info_file_name, 'w') as f:\n",
    "        f.write(\n",
    "            html_prefix + df[df[\"predicted_label\"] != df[\"actual_label\"]].sort_values(\"actual_label\").replace(\n",
    "                {r'\\n': '__NEWLINE__'}, regex=True\n",
    "            ).applymap(str).applymap(html.escape).replace(\n",
    "                {r'__NEWLINE__': '<br>'}, regex=True\n",
    "            ).style.set_table_styles(\n",
    "                [\n",
    "                    dict(selector=\"tr:nth-child(even)\", props=[(\"background-color\", \"#f2f2f2\")]),\n",
    "                    dict(selector=\"tr:nth-child(odd)\", props=[(\"background-color\", \"white\")]),                    \n",
    "                ]\n",
    "            ).render(\n",
    "                index=False, escape=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    link = f'<a href=\"{iteration_info_file_name}\" target=\"_blank\">{iteration_info_file_name}</a>'\n",
    "    display(HTML(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138725e1",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1647bb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-001.html\" target=\"_blank\">html_output/iteration-classification-001.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-001-diff.html\" target=\"_blank\">html_output/iteration-classification-001-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-002.html\" target=\"_blank\">html_output/iteration-classification-002.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-002-diff.html\" target=\"_blank\">html_output/iteration-classification-002-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-003.html\" target=\"_blank\">html_output/iteration-classification-003.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-003-diff.html\" target=\"_blank\">html_output/iteration-classification-003-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-004.html\" target=\"_blank\">html_output/iteration-classification-004.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-004-diff.html\" target=\"_blank\">html_output/iteration-classification-004-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/prompt-history-classification.html\" target=\"_blank\">html_output/prompt-history-classification.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-005.html\" target=\"_blank\">html_output/iteration-classification-005.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"html_output/iteration-classification-005-diff.html\" target=\"_blank\">html_output/iteration-classification-005-diff.html</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_parameters_history = [{k:v for k,v in model_parameters.items()}]\n",
    "parameters_update_response_history = []\n",
    "metrics_history = []\n",
    "\n",
    "for iteration_idx in range(1, NUM_ITERATIONS+1):\n",
    "    samples, actual_labels = get_texts_and_labels(dataset)\n",
    "\n",
    "    model_responses, predicted_labels = forward_pass(samples, model_parameters)\n",
    "    metrics = calculate_metrics(predicted_labels, actual_labels)\n",
    "    metrics_history.append(metrics)\n",
    "\n",
    "    if iteration_idx != NUM_ITERATIONS:  # don't update parameters for the last iteration\n",
    "        model_parameters, parameters_update_response = update_model_parameters(\n",
    "            samples, model_responses, predicted_labels, actual_labels, model_parameters, metrics)\n",
    "        parameters_update_response_history.append(parameters_update_response)\n",
    "        model_parameters_history.append({k:v for k,v in model_parameters.items()})\n",
    "\n",
    "    save_and_display_prompt_history(model_parameters_history, parameters_update_response_history, metrics_history)\n",
    "    save_and_display_current_iteration(iteration_idx, samples, model_responses, predicted_labels, actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188d56b",
   "metadata": {},
   "source": [
    "# Cost tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9154d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'haiku_input': 462669,\n",
       "             'haiku_output': 76423,\n",
       "             'o1_input': 22164,\n",
       "             'o1_output': 9344})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e509034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.42\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cost: ${sum(token_costs.values()):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede29994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
